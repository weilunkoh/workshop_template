import os
import tempfile

import lancedb
import openai
import streamlit as st
from langchain.chains import LLMChain
from langchain.chat_models import ChatOpenAI

# exercise 13
from langchain.document_loaders import PyPDFLoader, TextLoader
from langchain.embeddings.openai import OpenAIEmbeddings

# exercise 11
from langchain.llms import OpenAI

# exercise 12
from langchain.memory import ConversationBufferWindowMemory
from langchain.prompts import PromptTemplate
from langchain.vectorstores import LanceDB
from PIL import Image

# import from part 1
from part1 import ch10

# os.environ["OPENAI_API_KEY"] = st.secrets["openapi_key"]
# openai.api_key = st.secrets["openapi_key"]

# Global ex 13
cwd = os.getcwd()
WORKING_DIRECTORY = os.path.join(cwd, "database")

if not os.path.exists(WORKING_DIRECTORY):
    os.makedirs(WORKING_DIRECTORY)


def ex11a():  # change in ex11a
    # langchain prompt template
    prompt = PromptTemplate(
        input_variables=["subject", "topic"],
        template="""Design a lesson plan on {subject} on the topic of {topic} for primary 1 students""",
    )

    # openai_api_key = st.secrets["openapi_key"]
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.9)

    # creating a LLM chain with the langchain call and prompt template
    chain = LLMChain(llm=llm, prompt=prompt)
    if st.button("Run my chain"):
        input_prompt = prompt.format(subject="English", topic="Verbs")
        # Showing what is sent to LLM Chain
        st.write("Input prompt: ", input_prompt)
        # Showing the output from LLM Chain
        st.write(chain.run({"subject": "English", "topic": "Verbs"}))


def prompt_inputs_form():  # Using st.form, create the starting prompt to your prompt template, this is an expert on a topic that is talking to a user of a certain age
    # langchain prompt template
    with st.form("Prompt Template"):
        occupation = st.text_input("Enter the occupation:")
        topic = st.text_input("Enter the topic:")
        age = st.text_input("Enter the age:")

        # Every form must have a submit button.
        submitted = st.form_submit_button("Submit")
    # return a dictionary of the values
    if submitted:
        return {"occupation": occupation, "topic": topic, "age": age}


def ex11b():
    # create your template
    prompt_template = PromptTemplate(
        input_variables=["occupation", "topic", "age"],
        template="""Imagine you are a {occupation} who is an expert on the  topic of {topic} , you are going to help , teach and provide information to the person who is {age} years old, if you do not not know the answer, you must tell the person , do not make any answer up""",
    )
    # create a langchain function call to openai
    llm = ChatOpenAI(
        model_name="gpt-3.5-turbo",
        temperature=0.9,
    )
    # create a LLM chain with the langchain call and prompt template
    chain = LLMChain(llm=llm, prompt=prompt_template)
    # call the prompt_inputs_form()
    dict_inputs = prompt_inputs_form()
    if dict_inputs:
        response = chain.run(dict_inputs)
        with st.chat_message("assistant"):
            st.markdown(response)


# Challenge 11 Self Attempt
# def ch11(with_conversation_buffer=False):
#     # instead of running of the langchain, we are going to use the prompt template and run it the chatbot using format
#     prompt_template = PromptTemplate(
#         input_variables=["occupation", "topic", "age"],
#         template="""Imagine you are a {occupation} who is an expert on the  topic of {topic} , you are going to help , teach and provide information to the person who is {age} years old, if you do not not know the answer, you must tell the person , do not make any answer up""",
#     )
#     dict_inputs = prompt_inputs_form()
#     if dict_inputs:
#         input_prompt = prompt_template.format(
#             occupation=dict_inputs["occupation"],
#             topic=dict_inputs["topic"],
#             age=dict_inputs["age"],
#         )
#         # set session_state.prompt_template
#         st.session_state.prompt_template = input_prompt
#         st.write(
#             "New session_state.prompt_template: ", st.session_state.prompt_template
#         )

#     if with_conversation_buffer:
#         ch12_memory(input_prompt=st.session_state.prompt_template)
#     # call the ch10() basebot with the new session_state.prompt_template
#     ch10(
#         prompt_template=st.session_state.prompt_template,
#         store_memory=with_conversation_buffer,
#     )


def ch11():
    # instead of running of the langchain, we are going to use the prompt template and run it the chatbot using format
    prompt_template = PromptTemplate(
        input_variables=["occupation", "topic", "age"],
        template="""Imagine you are a {occupation} who is an expert on the  topic of {topic} , you are going to help , teach and provide information to the person who is {age} years old, if you do not not know the answer, you must tell the person , do not make any answer up""",
    )
    dict_inputs = prompt_inputs_form()
    if dict_inputs:
        input_prompt = prompt_template.format(
            occupation=dict_inputs["occupation"],
            topic=dict_inputs["topic"],
            age=dict_inputs["age"],
        )
        # set session_state.prompt_template
        st.session_state.prompt_template = input_prompt
        st.write("New session_state.prompt_template: ", input_prompt)
    # call the ch10() basebot with the new session_state.prompt_template
    ch10()


def ex12():
    memory = ConversationBufferWindowMemory(k=3)
    memory.save_context({"input": "hi"}, {"output": "whats up?"})
    memory.save_context({"input": "not much"}, {"output": "what can I help you with?"})

    st.write(memory.load_memory_variables({}))

    memory = ConversationBufferWindowMemory(k=3, return_messages=True)
    memory.save_context({"input": "hi"}, {"output": "whats up?"})
    memory.save_context({"input": "not much"}, {"output": "what can I help you with?"})

    st.write(memory.load_memory_variables({}))


# WL Self Attempt
# def ch12_memory(input_prompt):
#     if "memory" not in st.session_state:
#         st.session_state.memory = ConversationBufferWindowMemory(k=5)

#         # step 1 save the memory from your chatbot
#         # step 2 integrate the memory in the prompt_template (st.session_state.prompt_template)
#         memory_data = st.session_state.memory.load_memory_variables({})
#         st.write(memory_data)
#         st.session_state.prompt_template = f"""{input_prompt}

#     Below is the conversation history between the AI and Users so far

#     {memory_data}"""


# def ch12():
#     ch11(with_conversation_buffer=True)


def ch12():
    # Prompt_template form from ex11
    prompt_template = PromptTemplate(
        input_variables=["occupation", "topic", "age"],
        template="""Imagine you are a {occupation} who is an expert on the  topic of {topic} , you are going to help , teach and provide information
						to the person who is {age} years old, if you do not not know the answer, you must tell the person , do not make any answer up""",
    )
    dict_inputs = prompt_inputs_form()
    if dict_inputs:
        input_prompt = prompt_template.format(
            occupation=dict_inputs["occupation"],
            topic=dict_inputs["topic"],
            age=dict_inputs["age"],
        )
    else:
        input_prompt = "You are a helpful assistant. "

    st.write("input prompt: ", input_prompt)

    if "memory" not in st.session_state:
        st.session_state.memory = ConversationBufferWindowMemory(k=3)

    # step 1 save the memory from your chatbot
    # step 2 integrate the memory in the prompt_template (st.session_state.prompt_template) show a hint
    memory_data = st.session_state.memory.load_memory_variables({})
    st.write("Memory Data: ", memory_data)
    st.session_state.prompt_template = f"""
{input_prompt}										

Below is the conversation history between the AI and Users so far

{memory_data}

"""

    st.write("New prompt template: ", st.session_state.prompt_template)
    # call the function in your base bot
    # Initialize chat history
    if "msg" not in st.session_state:
        st.session_state.msg = []

    # Showing Chat history
    for message in st.session_state.msg:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    try:
        #
        if prompt := st.chat_input("What is up?"):
            # set user prompt in chat history
            st.session_state.msg.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                full_response = ""
                # streaming function
                for response in chat_completion_stream_prompt(prompt):
                    full_response += response.choices[0].delta.get("content", "")
                    message_placeholder.markdown(full_response + "â–Œ")
                message_placeholder.markdown(full_response)
            st.session_state.msg.append({"role": "assistant", "content": full_response})
            st.session_state.memory.save_context(
                {"input": prompt}, {"output": full_response}
            )
    except Exception as e:
        st.error(e)


# exercise 13 - loading
def upload_file_streamlit():
    def get_file_extension(file_name):
        return os.path.splitext(file_name)[1]

    st.subheader("Upload your docs")

    # Streamlit file uploader to accept file input
    uploaded_file = st.file_uploader("Choose a file", type=["docx", "txt", "pdf"])

    if uploaded_file:
        # Reading file content
        file_content = uploaded_file.read()

        # Determine the suffix based on uploaded file's name
        file_suffix = get_file_extension(uploaded_file.name)

        # Saving the uploaded file temporarily to process it
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_suffix) as temp_file:
            temp_file.write(file_content)
            temp_file.flush()  # Ensure the data is written to the file
            temp_file_path = temp_file.name
        return temp_file_path


# exercise 13 - split and chunk, embeddings and storing in vectorstores for reference
def vectorstore_creator():
    # WORKING_DIRECTORY set above in the main.py
    # Process the temporary file using UnstructuredFileLoader (or any other method you need)
    embeddings = OpenAIEmbeddings()
    db = lancedb.connect(WORKING_DIRECTORY)
    table = db.create_table(
        "my_table",
        data=[
            {
                "vector": embeddings.embed_query("Query unsuccessful"),
                "text": "Query unsuccessful",
                "id": "1",
            }
        ],
        mode="overwrite",
    )
    # st.write(temp_file_path)
    temp_file_path = upload_file_streamlit()
    if temp_file_path:
        loader = PyPDFLoader(temp_file_path)
        documents = loader.load_and_split()
        db = LanceDB.from_documents(documents, embeddings, connection=table)
        return db


def ex13():
    if "vectorstore" not in st.session_state:
        st.session_state.vectorstore = False
    db = vectorstore_creator()
    st.session_state.vectorstore = db
    if st.session_state.vectorstore:
        query = st.text_input("Enter a query")
        if query:
            st.session_state.vectorstore = db
            docs = db.similarity_search(query)
            st.write(docs[0].page_content)


def chat_completion_stream_prompt(prompt):
    MODEL = "gpt-3.5-turbo"  # consider changing this to session_state
    response = openai.ChatCompletion.create(
        model=MODEL,
        messages=[
            {"role": "system", "content": st.session_state.prompt_template},
            {"role": "user", "content": prompt},
        ],
        temperature=0,  # temperature
        stream=True,  # stream option
    )
    return response


# save the vectorstore in st.session_state
# add semantic search prompt into memory prompt
# integrate back into your chatbot
def ex14_basebot():
    # Prompt_template form from ex11
    prompt_template = PromptTemplate(
        input_variables=["occupation", "topic", "age"],
        template="""Imagine you are a {occupation} who is an expert on the  topic of {topic} , you are going to help , teach and provide information
						to the person who is {age} years old, if you do not not know the answer, you must tell the person , do not make any answer up""",
    )
    dict_inputs = prompt_inputs_form()
    if dict_inputs:
        input_prompt = prompt_template.format(
            occupation=dict_inputs["occupation"],
            topic=dict_inputs["topic"],
            age=dict_inputs["age"],
        )
        st.session_state.input_prompt = input_prompt

    if "input_prompt" not in st.session_state:
        st.session_state.input_prompt = "Speak like Yoda from Star Wars"

    if "memory" not in st.session_state:
        st.session_state.memory = ConversationBufferWindowMemory(k=5)

    # step 1 save the memory from your chatbot
    # step 2 integrate the memory in the prompt_template (st.session_state.prompt_template) show a hint
    memory_data = st.session_state.memory.load_memory_variables({})
    st.write(memory_data)
    st.session_state.prompt_template = f"""
st.session_state.input_prompt: {st.session_state.input_prompt}

This is the last conversation history
{memory_data}

"""
    st.write("new prompt template: ", st.session_state.prompt_template)

    st.session_state.vectorstore = vectorstore_creator()

    # Initialize chat history
    if "msg" not in st.session_state:
        st.session_state.msg = []

    # Showing Chat history
    for message in st.session_state.msg:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    try:
        #
        if prompt := st.chat_input("What is up?"):
            # query information
            if st.session_state.vectorstore:
                docs = st.session_state.vectorstore.similarity_search(prompt)
                docs = docs[0].page_content
                # add your query prompt
                vs_prompt = f"""You should reference this search result to help your answer,
								{docs}
								if the search result does not anwer the query, please say you are unable to answer, do not make up an answer"""
            else:
                vs_prompt = ""
            # add query prompt to your memory prompt and send it to LLM
            st.session_state.prompt_template = (
                st.session_state.prompt_template + vs_prompt
            )
            # set user prompt in chat history
            st.session_state.msg.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                full_response = ""
                # streaming function
                for response in chat_completion_stream_prompt(prompt):
                    full_response += response.choices[0].delta.get("content", "")
                    message_placeholder.markdown(full_response + "â–Œ")
                message_placeholder.markdown(full_response)
            st.session_state.msg.append({"role": "assistant", "content": full_response})
            st.session_state.memory.save_context(
                {"input": prompt}, {"output": full_response}
            )

    except Exception as e:
        st.error(e)
